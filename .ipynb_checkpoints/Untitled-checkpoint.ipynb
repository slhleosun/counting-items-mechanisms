{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361adbc7-bfb1-4636-9091-3c654c939cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset…\n",
      "  → 200 prompts selected for analysis.\n",
      "Parsed 200 prompts successfully.\n",
      "\n",
      "Loading model meta-llama/Meta-Llama-3-8B-Instruct on cuda…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec75ea5655054b109f4c26f34028118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# 3.  INITIALIZE MODEL & TOKENIZER (TransformerLens)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m model \u001b[38;5;241m=\u001b[39m HookedTransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    181\u001b[0m     MODEL_NAME,\n\u001b[1;32m    182\u001b[0m     center_unembed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,     \n\u001b[1;32m    183\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m    184\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,            \n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# 4.  CACHE CLEAN RUNS (ENTIRE PROMPT) & PREPARE COUNTERFACTUAL RUNS\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# 4a.  Cache the CLEAN run (entire prompt) & compute running counts\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1322\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     center_unembed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# match the HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_pretrained_state_dict(\n\u001b[1;32m   1323\u001b[0m     official_model_name, cfg, hf_model, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs\n\u001b[1;32m   1324\u001b[0m )\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m   1328\u001b[0m     cfg,\n\u001b[1;32m   1329\u001b[0m     tokenizer,\n\u001b[1;32m   1330\u001b[0m     move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1331\u001b[0m     default_padding_side\u001b[38;5;241m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1332\u001b[0m )\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:1794\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         hf_model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   1788\u001b[0m             official_model_name,\n\u001b[1;32m   1789\u001b[0m             torch_dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1790\u001b[0m             token\u001b[38;5;241m=\u001b[39mhuggingface_token,\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1792\u001b[0m         )\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1794\u001b[0m         hf_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   1795\u001b[0m             official_model_name,\n\u001b[1;32m   1796\u001b[0m             torch_dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1797\u001b[0m             token\u001b[38;5;241m=\u001b[39mhuggingface_token,\n\u001b[1;32m   1798\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1799\u001b[0m         )\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;66;03m# Load model weights, and fold in layer norm weights\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m hf_model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4224\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4214\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4217\u001b[0m         (\n\u001b[1;32m   4218\u001b[0m             model,\n\u001b[1;32m   4219\u001b[0m             missing_keys,\n\u001b[1;32m   4220\u001b[0m             unexpected_keys,\n\u001b[1;32m   4221\u001b[0m             mismatched_keys,\n\u001b[1;32m   4222\u001b[0m             offload_index,\n\u001b[1;32m   4223\u001b[0m             error_msgs,\n\u001b[0;32m-> 4224\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   4225\u001b[0m             model,\n\u001b[1;32m   4226\u001b[0m             state_dict,\n\u001b[1;32m   4227\u001b[0m             loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   4228\u001b[0m             resolved_archive_file,\n\u001b[1;32m   4229\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   4230\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   4231\u001b[0m             sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   4232\u001b[0m             _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   4233\u001b[0m             low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   4234\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4235\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4236\u001b[0m             offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   4237\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4238\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4239\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4240\u001b[0m             gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[1;32m   4241\u001b[0m             weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   4242\u001b[0m         )\n\u001b[1;32m   4244\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4245\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4818\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4814\u001b[0m         assign_to_params_buffers \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(\n\u001b[1;32m   4815\u001b[0m             model_to_load, state_dict, start_prefix\n\u001b[1;32m   4816\u001b[0m         )\n\u001b[1;32m   4817\u001b[0m     fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4818\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _load_state_dict_into_model(\n\u001b[1;32m   4819\u001b[0m         model_to_load, fixed_state_dict, start_prefix, assign_to_params_buffers\n\u001b[1;32m   4820\u001b[0m     )\n\u001b[1;32m   4822\u001b[0m \u001b[38;5;66;03m# force memory release\u001b[39;00m\n\u001b[1;32m   4823\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/modeling_utils.py:694\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n\u001b[0;32m--> 694\u001b[0m load(model_to_load, state_dict, prefix\u001b[38;5;241m=\u001b[39mstart_prefix, assign_to_params_buffers\u001b[38;5;241m=\u001b[39massign_to_params_buffers)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/modeling_utils.py:692\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 692\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/modeling_utils.py:688\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[1;32m    686\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m         module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:2096\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2096\u001b[0m             param\u001b[38;5;241m.\u001b[39mcopy_(input_param)\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2098\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "counting_causal_prefix_permute.py\n",
    "\n",
    "A full pipeline that:\n",
    "  1. Loads your `count_match_dataset.json`.\n",
    "  2. For each example i and each prefix-index k:\n",
    "     • Builds a “prefix-permuted” counterfactual list:\n",
    "         ‣ prefix = word_list[0 : k+1] shuffled\n",
    "         ‣ suffix = word_list[k+1 : ] (unchanged)\n",
    "  3. Runs the original (“clean”) prompt once and caches all hidden states.\n",
    "  4. Runs each “prefix-permuted” prompt once (for every k) and caches those hidden states.\n",
    "  5. Trains a ridge‐regression probe, layer by layer, on `(h_clean[ℓ,k] → running_count(k))`.\n",
    "     Chooses the best‐scoring layer ℓ*.\n",
    "  6. For every (i, k), patches layer ℓ* by swapping in `h_cf[ℓ*, k]` (the counterfactual prefix’s state)\n",
    "     into the clean run, then completes the forward pass to get a new final integer prediction.\n",
    "  7. Visualizes how often / by how much the patched run’s answer shifts toward the counterfactual’s answer.\n",
    "\n",
    "Note: Here, “counterfactual” means “shuffle the first k+1 words,” not shuffling the suffix.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TransformerLens imports\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  CONFIGURATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DEVICE = \"cuda\"                 # or \"cpu\"\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "DATA_PATH = Path(\"dataset.json\")\n",
    "MAX_EXAMPLES = 200              # Set to None to use all examples\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "CATEGORY_ITEMS = {\n",
    "    \"fruit\": [\n",
    "        \"apple\", \"banana\", \"cherry\", \"grape\", \"orange\",\n",
    "        \"pear\", \"peach\", \"mango\", \"tangerine\", \"plum\"\n",
    "    ],\n",
    "    \"animal\": [\n",
    "        \"dog\", \"cat\", \"horse\", \"cow\", \"sheep\",\n",
    "        \"lion\", \"tiger\", \"bear\", \"rabbit\", \"fox\"\n",
    "    ],\n",
    "    \"vehicle\": [\n",
    "        \"car\", \"bus\", \"truck\", \"bicycle\", \"motorcycle\",\n",
    "        \"train\", \"boat\", \"plane\", \"scooter\", \"van\"\n",
    "    ],\n",
    "    \"instrument\": [\n",
    "        \"guitar\", \"piano\", \"violin\", \"drum\", \"flute\",\n",
    "        \"trumpet\", \"saxophone\", \"cello\", \"clarinet\", \"harp\"\n",
    "    ],\n",
    "    \"furniture\": [\n",
    "        \"chair\", \"table\", \"sofa\", \"bed\", \"desk\",\n",
    "        \"cabinet\", \"dresser\", \"stool\", \"wardrobe\", \"bookshelf\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  HELPERS: Parsing prompts & building “prefix-permuted” runs\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def parse_prompt(prompt_str):\n",
    "    \"\"\"\n",
    "    Extracts:\n",
    "      • word_type  (e.g. \"fruit\")\n",
    "      • word_list  (list of strings inside the brackets)\n",
    "      • word_positions (list of (char_start, char_end) for each word)\n",
    "    \"\"\"\n",
    "    m_type = re.search(r\"Type:\\s*(\\w+)\", prompt_str)\n",
    "    if not m_type:\n",
    "        raise ValueError(\"Could not find ‘Type:’ in prompt.\")\n",
    "    word_type = m_type.group(1).strip()\n",
    "\n",
    "    idx_list = prompt_str.find(\"List:\")\n",
    "    if idx_list < 0:\n",
    "        raise ValueError(\"Could not find ‘List:’ in prompt.\")\n",
    "    start_bracket = prompt_str.find(\"[\", idx_list)\n",
    "    end_bracket = prompt_str.find(\"]\", start_bracket)\n",
    "    if start_bracket < 0 or end_bracket < 0:\n",
    "        raise ValueError(\"Could not find matching brackets for ‘List:’.\")\n",
    "\n",
    "    list_substr = prompt_str[start_bracket+1 : end_bracket].strip()\n",
    "    word_list = [w.strip() for w in list_substr.split(\",\") if w.strip()]\n",
    "\n",
    "    # For each word, find its char-span in the prompt\n",
    "    list_positions = []\n",
    "    cursor = start_bracket + 1\n",
    "    for w in word_list:\n",
    "        idx = prompt_str.find(w, cursor)\n",
    "        if idx < 0:\n",
    "            idx = prompt_str.lower().find(w.lower(), cursor)\n",
    "            if idx < 0:\n",
    "                raise ValueError(f\"Could not locate word '{w}' in prompt.\")\n",
    "        list_positions.append((idx, idx + len(w)))\n",
    "        cursor = idx + len(w)\n",
    "\n",
    "    return word_type, word_list, list_positions\n",
    "\n",
    "\n",
    "def build_prefix_permuted_list(word_list, k):\n",
    "    \"\"\"\n",
    "    Given an original word_list of length N, and a prefix-index k (0 ≤ k < N),\n",
    "    return a new list where:\n",
    "      • prefix = word_list[0 : k+1] is randomly shuffled\n",
    "      • suffix = word_list[k+1 : N] is left unchanged\n",
    "    \"\"\"\n",
    "    prefix = word_list[: k + 1].copy()\n",
    "    random.shuffle(prefix)\n",
    "    suffix = word_list[k + 1 :].copy()\n",
    "    return prefix + suffix\n",
    "\n",
    "\n",
    "def reconstruct_prompt(prompt_str, new_list):\n",
    "    \"\"\"\n",
    "    Given the original prompt_str and a new_list of the same length,\n",
    "    replace the text inside “List: [ ... ]” with “, ”-joined new_list.\n",
    "    \"\"\"\n",
    "    idx_list = prompt_str.find(\"List:\")\n",
    "    start_bracket = prompt_str.find(\"[\", idx_list)\n",
    "    end_bracket = prompt_str.find(\"]\", start_bracket)\n",
    "    if start_bracket < 0 or end_bracket < 0:\n",
    "        raise ValueError(\"Could not find List brackets in prompt.\")\n",
    "\n",
    "    before = prompt_str[: start_bracket + 1]\n",
    "    after = prompt_str[end_bracket:]\n",
    "    inside = \", \".join(new_list)\n",
    "    return before + inside + after\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  LOAD & FILTER DATA\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"Loading dataset…\")\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "if MAX_EXAMPLES is not None and len(all_data) > MAX_EXAMPLES:\n",
    "    all_data = random.sample(all_data, MAX_EXAMPLES)\n",
    "print(f\"  → {len(all_data)} prompts selected for analysis.\")\n",
    "\n",
    "parsed = []\n",
    "for rec in all_data:\n",
    "    prompt = rec[\"prompt\"]\n",
    "    try:\n",
    "        wtype, wlist, wpos = parse_prompt(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping prompt due to parse error: {e}\")\n",
    "        continue\n",
    "    parsed.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"type\": wtype,\n",
    "        \"word_list\": wlist,\n",
    "        \"word_positions\": wpos,\n",
    "        \"gold_answer\": rec[\"answer\"],\n",
    "    })\n",
    "print(f\"Parsed {len(parsed)} prompts successfully.\\n\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  INITIALIZE MODEL & TOKENIZER (TransformerLens)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(f\"Loading model {MODEL_NAME} on {DEVICE}…\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,     \n",
    "    device=DEVICE,\n",
    "    fold_ln=True,            \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  CACHE CLEAN RUNS (ENTIRE PROMPT) & PREPARE COUNTERFACTUAL RUNS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4a.  Cache the CLEAN run (entire prompt) & compute running counts\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"Building caches…\")\n",
    "cache_clean       = {}   # i → cache on the CLEAN prompt\n",
    "cache_positions   = {}   # i → list of token indices aligned to each word\n",
    "cache_word_counts = {}   # i → [c₀, c₁, …], running count on the clean prefix\n",
    "\n",
    "for i, rec in enumerate(tqdm(parsed, desc=\"CACHING CLEAN\")):\n",
    "    prompt = rec[\"prompt\"]\n",
    "    wlist  = rec[\"word_list\"]\n",
    "    wpos   = rec[\"word_positions\"]\n",
    "    wtype  = rec[\"type\"]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    token_ids = enc[\"input_ids\"][0].cpu().tolist()\n",
    "    offsets   = enc[\"offset_mapping\"][0].cpu().tolist()  # shape: (seq_len, 2)\n",
    "\n",
    "    # Align each word to the token index with the greatest character overlap\n",
    "    word_to_token_idx = []\n",
    "    for (char_start, char_end) in wpos:\n",
    "        best_tidx = None\n",
    "        best_overlap = 0\n",
    "        for tidx, (ts, te) in enumerate(offsets):\n",
    "            overlap = min(te, char_end) - max(ts, char_start)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_tidx = tidx\n",
    "        if best_tidx is None or best_overlap <= 0:\n",
    "            raise RuntimeError(f\"Could not align word at chars {(char_start, char_end)}\")\n",
    "        word_to_token_idx.append(best_tidx)\n",
    "    cache_positions[i] = word_to_token_idx\n",
    "\n",
    "    # Build true running‐count array using CATEGORY_ITEMS\n",
    "    pool_lower = {x.lower() for x in CATEGORY_ITEMS[wtype]}\n",
    "    c_running = []\n",
    "    count_so_far = 0\n",
    "    for w in wlist:\n",
    "        if w.lower() in pool_lower:\n",
    "            count_so_far += 1\n",
    "        c_running.append(count_so_far)\n",
    "    cache_word_counts[i] = c_running\n",
    "\n",
    "    # Full forward pass, caching all resid_post\n",
    "    _, cache = model.run_with_cache(enc[\"input_ids\"], return_type=\"both\")\n",
    "    # Move every tensor in `cache` to CPU, then clear it from GPU\n",
    "    for key, tensor in cache.items():\n",
    "        cache[key] = tensor.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    cache_clean[i] = cache\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4b.  Cache each “prefix-permuted” counterfactual (i, k)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "cache_cf = {}  # (i, k) → cache object for the prefix‐permuted prompt\n",
    "\n",
    "for i, rec in enumerate(tqdm(parsed, desc=\"CACHING COUNTERFACTUAL\")):\n",
    "    original_prompt = rec[\"prompt\"]\n",
    "    wlist = rec[\"word_list\"]\n",
    "\n",
    "    for k in range(len(wlist)):\n",
    "        cf_wlist = build_prefix_permuted_list(wlist, k)\n",
    "        cf_prompt = reconstruct_prompt(original_prompt, cf_wlist)\n",
    "\n",
    "        enc_cf = tokenizer(\n",
    "            cf_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Align each word in the *prefix-permuted* list to token indices\n",
    "        # (repeat the same overlap‐based logic so we know where to patch later)\n",
    "        offsets_cf = enc_cf[\"offset_mapping\"][0].cpu().tolist()\n",
    "        wpos_cf = []  # but word_positions stayed the same character-wise,\n",
    "                      # since only the order of words changed inside the brackets.\n",
    "        # Actually, wpos (from the clean prompt) no longer matches cf_prompt exactly\n",
    "        # if character offsets shift. But we know that the bracketed “List” substring\n",
    "        # is identical length, only words change order. So we can re‐extract wpos_cf:\n",
    "        #\n",
    "        #   1. Use parse_prompt(cf_prompt) to get fresh wpos_cf\n",
    "        #   2. Then apply the same overlap‐based matching.\n",
    "        #\n",
    "        _, _, wpos_cf = parse_prompt(cf_prompt)\n",
    "\n",
    "        word_to_token_idx_cf = []\n",
    "        for (char_start, char_end) in wpos_cf:\n",
    "            best_tidx = None\n",
    "            best_overlap = 0\n",
    "            for tidx, (ts, te) in enumerate(offsets_cf):\n",
    "                overlap = min(te, char_end) - max(ts, char_start)\n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap = overlap\n",
    "                    best_tidx = tidx\n",
    "            if best_tidx is None or best_overlap <= 0:\n",
    "                raise RuntimeError(f\"Could not align CF word at chars {(char_start, char_end)}\")\n",
    "            word_to_token_idx_cf.append(best_tidx)\n",
    "\n",
    "        # (We only need word_to_token_idx_cf later if we want to double-check patching positions.\n",
    "        #  For now, we stash the entire cache so we can look up resid_post at (ℓ, token_idx_cf[k]).)\n",
    "\n",
    "        _, cacheobj_cf = model.run_with_cache(enc_cf[\"input_ids\"], return_type=\"both\")\n",
    "        # Move every tensor in cacheobj_cf to CPU\n",
    "        for key, tensor in cacheobj_cf.items():\n",
    "            cacheobj_cf[key] = tensor.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        cache_cf[(i, k)] = {\n",
    "            \"cache\": cacheobj_cf,\n",
    "            \"token_idxs\": word_to_token_idx_cf\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  TRAIN LAYERWISE PROBES TO LOCATE ℓ*\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "LAYERS = list(range(model.cfg.n_layers))\n",
    "layer_scores = []\n",
    "\n",
    "print(\"\\n=== Training probes layer-by-layer ===\")\n",
    "for ℓ in tqdm(LAYERS, desc=\"TRAIN PROBE\"):\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    for i, rec in enumerate(parsed):\n",
    "        c_running = cache_word_counts[i]\n",
    "        tok_idxs   = cache_positions[i]\n",
    "        cacheobj   = cache_clean[i]\n",
    "        for k, c_k in enumerate(c_running):\n",
    "            token_index = tok_idxs[k]\n",
    "            h_lk = cacheobj[(\"resid_post\", ℓ)][0, token_index, :].cpu().numpy()\n",
    "            X_all.append(h_lk)\n",
    "            y_all.append(c_k)\n",
    "\n",
    "    X_all = np.stack(X_all)\n",
    "    y_all = np.array(y_all)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=0.2, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    ridge = Ridge(alpha=1e-3)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    r2 = ridge.score(X_test, y_test)\n",
    "    layer_scores.append((ℓ, r2))\n",
    "\n",
    "layer_scores = sorted(layer_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nProbe R² by layer (top 5):\")\n",
    "for ℓ, score in layer_scores[:5]:\n",
    "    print(f\"  Layer {ℓ:02d} → R² = {score:.4f}\")\n",
    "\n",
    "ℓ_star = layer_scores[0][0]\n",
    "print(f\"\\n==> Selected layer ℓ* = {ℓ_star}\")\n",
    "\n",
    "# Plot R² vs. layer index\n",
    "plt.figure(figsize=(6,4))\n",
    "layers_idx = [ℓ for ℓ,_ in layer_scores]\n",
    "scores_val = [r2 for _,r2 in layer_scores]\n",
    "plt.plot(layers_idx, scores_val, marker=\"o\")\n",
    "plt.xlabel(\"Layer index ℓ\")\n",
    "plt.ylabel(\"Probe R²\")\n",
    "plt.title(\"Linear‐Probe R² for running‐count (clean run)\")\n",
    "plt.axvline(ℓ_star, linestyle=\"--\", color=\"red\", label=f\"ℓ* = {ℓ_star}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6.  COMPUTE “CLEAN” & “CF” FINAL-INTEGER PREDICTIONS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 6a. Helper to decode the model’s final integer from logits\n",
    "def decode_integer_from_logits(logits_np, topk=200):\n",
    "    \"\"\"\n",
    "    Given a 1D array of logits (vocab_size,), pick the highest‐scoring token whose text is all digits.\n",
    "    Returns an int or raises if none found.\n",
    "    \"\"\"\n",
    "    best_digit = None\n",
    "    best_score = -1e9\n",
    "    for token_id in np.argsort(logits_np)[-topk:]:\n",
    "        token_str = tokenizer.decode([token_id]).strip()\n",
    "        if re.fullmatch(r\"\\d+\", token_str):\n",
    "            score = logits_np[token_id]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_digit = int(token_str)\n",
    "    if best_digit is None:\n",
    "        raise RuntimeError(\"Could not parse final integer from logits.\")\n",
    "    return best_digit\n",
    "\n",
    "\n",
    "# 6b. Compute clean final prediction y_clean(i) for each example i\n",
    "print(\"\\n=== Computing clean final predictions ===\")\n",
    "clean_final_preds = {}\n",
    "for i, rec in enumerate(parsed):\n",
    "    enc = tokenizer.encode(rec[\"prompt\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = model(enc[\"input_ids\"])   # returns (1, seq_len, vocab_size)\n",
    "    logits_np = logits[:, -1, :].cpu().numpy().flatten()\n",
    "    clean_final_preds[i] = decode_integer_from_logits(logits_np)\n",
    "\n",
    "# 6c. Compute counterfactual final prediction y_cf(i, k)\n",
    "#      (Here, since we permuted the prefix only, the total count remains the same.\n",
    "#       But we still compute to be consistent.)\n",
    "print(\"Computing counterfactual final predictions…\")\n",
    "cf_final_preds = {}\n",
    "for (i, k), cacheobj_cf in tqdm(cache_cf.items(), desc=\"CF finals\"):\n",
    "    # Re‐decode by re‐running the forward pass (since TL’s cache doesn’t store logits by default)\n",
    "    rec = parsed[i]\n",
    "    cf_wlist = build_prefix_permuted_list(rec[\"word_list\"], k)\n",
    "    cf_prompt = reconstruct_prompt(rec[\"prompt\"], cf_wlist)\n",
    "    enc_cf = tokenizer.encode(cf_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits_cf = model(enc_cf[\"input_ids\"])\n",
    "    logits_np = logits_cf[:, -1, :].cpu().numpy().flatten()\n",
    "    cf_final_preds[(i, k)] = decode_integer_from_logits(logits_np)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7.  CAUSAL PATCHING AT ℓ*: SWAP h_clean[ℓ*,k] → h_cf[ℓ*,k]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(f\"\\n=== Running causal patching at layer ℓ* = {ℓ_star} ===\")\n",
    "\n",
    "# Pre‐tokenize all clean prompts once\n",
    "all_clean_enc = [\n",
    "    tokenizer.encode(rec[\"prompt\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    for rec in parsed\n",
    "]\n",
    "\n",
    "# Prepare a matrix Δ(i,k) = ŷ_patched(i,k) − ŷ_clean(i)\n",
    "delta_matrix = {\n",
    "    i: {k: None for k in range(len(parsed[i][\"word_list\"]))}\n",
    "    for i in range(len(parsed))\n",
    "}\n",
    "\n",
    "for i, rec in enumerate(parsed):\n",
    "    prompt_enc = all_clean_enc[i]\n",
    "    wlist = rec[\"word_list\"]\n",
    "    tok_idxs = cache_positions[i]\n",
    "\n",
    "    for k in range(len(wlist)):\n",
    "        # h_cf^(ℓ*, k) from the prefix‐permuted cache\n",
    "        h_cf_vec = cache_cf[(i, k)][(\"resid_post\", ℓ_star)][0, tok_idxs[k], :].detach().clone()\n",
    "\n",
    "        # Build a hook that replaces clean h^(ℓ*, k) with h_cf^(ℓ*, k)\n",
    "        def make_patch_hook(i_local, k_local, h_cf_local):\n",
    "            def patch_hook(resid, hook):\n",
    "                # resid shape: (1, seq_len, d_model)\n",
    "                token_index = tok_idxs[k_local]\n",
    "                resid[:, token_index, :] = h_cf_local\n",
    "                return resid\n",
    "            return patch_hook\n",
    "\n",
    "        patch_fn = make_patch_hook(i, k, h_cf_vec)\n",
    "\n",
    "        # Run the patched forward pass\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            prompt_enc[\"input_ids\"],\n",
    "            fwd_hooks={(\"resid_post\", ℓ_star): patch_fn},\n",
    "            return_type=\"logits\"\n",
    "        )[0]  # shape: (1, seq_len, vocab_size)\n",
    "\n",
    "        logits_np = patched_logits[:, -1, :].cpu().numpy().flatten()\n",
    "        pred_patched = decode_integer_from_logits(logits_np)\n",
    "        delta_matrix[i][k] = pred_patched - clean_final_preds[i]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8.  VISUALIZE RESULTS (HEATMAP + AVERAGE EFFECT)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Build normalized effect E(i,k) = Δ(i,k) / (cf_final(i,k) − clean_final(i))\n",
    "E_matrix = []\n",
    "for i in range(len(parsed)):\n",
    "    row = []\n",
    "    for k in range(len(parsed[i][\"word_list\"])):\n",
    "        denom = cf_final_preds[(i, k)] - clean_final_preds[i]\n",
    "        if denom == 0:\n",
    "            e = 0.0\n",
    "        else:\n",
    "            e = delta_matrix[i][k] / denom\n",
    "        row.append(e)\n",
    "    E_matrix.append(row)\n",
    "\n",
    "max_len = max(len(r) for r in E_matrix)\n",
    "E_padded = np.array([r + [np.nan]*(max_len - len(r)) for r in E_matrix])\n",
    "\n",
    "# (a) Heatmap\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(E_padded, aspect=\"auto\", vmin=0, vmax=1, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Normalized causal effect\\, E(i,k)\")\n",
    "plt.xlabel(\"Prefix index k\")\n",
    "plt.ylabel(\"Example index i\")\n",
    "plt.title(f\"Causal‐effect heatmap at layer ℓ* = {ℓ_star}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (b) Average effect vs. k\n",
    "avg_effect = np.nanmean(E_padded, axis=0)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(max_len), avg_effect, marker=\"o\")\n",
    "plt.xlabel(\"Prefix index k\")\n",
    "plt.ylabel(\"Mean normalized effect\\, E(k)\")\n",
    "plt.title(\"Average normalized causal effect vs. k\")\n",
    "plt.ylim(0,1)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPipeline complete. Summary:\")\n",
    "print(f\"  • Probe located ℓ* = {ℓ_star}\")\n",
    "print(\"  • Heatmap (E_padded) shows how patching at (ℓ*,k) shifts the final count toward the prefix-permuted counterfactual.\")\n",
    "\n",
    "# === END OF SCRIPT ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc6425-9e08-4fae-881c-b3fad336dae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixed_ai_env",
   "language": "python",
   "name": "fixed_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
