{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b11f0b-d88b-4201-91a2-3bce4bed5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cuda\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7495723835b4ac6afa446166178bf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 32 layers, hidden size 4096\n",
      "Loaded 5000 examples from dataset\n",
      "\n",
      "==================================================\n",
      "PROJECTION TRAINING EXPERIMENT\n",
      "==================================================\n",
      "\n",
      "Collecting hidden states for projection training...\n",
      "Collecting hidden states from layers 32 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:37<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "\n",
      "=== Training projection for layer 10 ===\n",
      "Collecting hidden states from layers 10 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:21<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.1046, LR: 0.000854\n",
      "Epoch 10/20, Loss: 0.0680, LR: 0.000500\n",
      "Epoch 15/20, Loss: 0.0486, LR: 0.000146\n",
      "Epoch 20/20, Loss: 0.0388, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 11 ===\n",
      "Collecting hidden states from layers 11 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.1067, LR: 0.000854\n",
      "Epoch 10/20, Loss: 0.0672, LR: 0.000500\n",
      "Epoch 15/20, Loss: 0.0476, LR: 0.000146\n",
      "Epoch 20/20, Loss: 0.0375, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 12 ===\n",
      "Collecting hidden states from layers 12 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0967, LR: 0.000854\n",
      "Epoch 10/20, Loss: 0.0637, LR: 0.000500\n",
      "Epoch 15/20, Loss: 0.0432, LR: 0.000146\n",
      "Epoch 20/20, Loss: 0.0334, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 13 ===\n",
      "Collecting hidden states from layers 13 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0562, LR: 0.000854\n",
      "Epoch 10/20, Loss: 0.0275, LR: 0.000500\n",
      "Epoch 15/20, Loss: 0.0102, LR: 0.000146\n",
      "Epoch 20/20, Loss: 0.0019, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 14 ===\n",
      "Collecting hidden states from layers 14 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0361, LR: 0.000854\n",
      "Epoch 10/20, Loss: 0.0098, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0071, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0153, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 15 ===\n",
      "Collecting hidden states from layers 15 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0223, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0047, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0238, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0328, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 16 ===\n",
      "Collecting hidden states from layers 16 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0219, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0041, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0222, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0310, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 17 ===\n",
      "Collecting hidden states from layers 17 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0226, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0029, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0199, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0283, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 18 ===\n",
      "Collecting hidden states from layers 18 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:23<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0124, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0211, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0381, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0458, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 19 ===\n",
      "Collecting hidden states from layers 19 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:22<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0181, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0150, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0343, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0425, LR: 0.000000\n",
      "\n",
      "=== Training projection for layer 20 ===\n",
      "Collecting hidden states from layers 20 and 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|███████████████████████████████████████████| 2000/2000 [01:22<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10000 hidden state pairs\n",
      "Training projection with 10000 samples...\n",
      "Epoch 5/20, Loss: 0.0150, LR: 0.000854\n",
      "Epoch 10/20, Loss: -0.0171, LR: 0.000500\n",
      "Epoch 15/20, Loss: -0.0359, LR: 0.000146\n",
      "Epoch 20/20, Loss: -0.0437, LR: 0.000000\n",
      "\n",
      "==================================================\n",
      "EVALUATION PHASE\n",
      "==================================================\n",
      "\n",
      "=== Evaluating Layer 10 with Projection ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 10 + projection:   0%|                                           | 0/5000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 546\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - projection_results.json: Detailed results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 500\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROJECTION TRAINING EXPERIMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m--> 500\u001b[0m results, projections \u001b[38;5;241m=\u001b[39m run_projection_experiment(\n\u001b[1;32m    501\u001b[0m     model, tokenizer, dataset,\n\u001b[1;32m    502\u001b[0m     start_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    503\u001b[0m     end_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    504\u001b[0m     train_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,  \u001b[38;5;66;03m# Number of examples for training projections\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     eval_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)  \u001b[38;5;66;03m# Evaluate on full dataset\u001b[39;00m\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 345\u001b[0m, in \u001b[0;36mrun_projection_experiment\u001b[0;34m(model, tokenizer, dataset, start_layer, end_layer, train_samples, eval_samples)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Evaluating Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with Projection ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m     accuracy, predictions \u001b[38;5;241m=\u001b[39m evaluate_with_projection(\n\u001b[1;32m    346\u001b[0m         model, tokenizer, dataset, \n\u001b[1;32m    347\u001b[0m         stop_at_layer\u001b[38;5;241m=\u001b[39mlayer,\n\u001b[1;32m    348\u001b[0m         projection\u001b[38;5;241m=\u001b[39mprojections[layer],\n\u001b[1;32m    349\u001b[0m         max_samples\u001b[38;5;241m=\u001b[39meval_samples\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m     results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m: layer,\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy,\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions\n\u001b[1;32m    356\u001b[0m     }\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy for Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m + Projection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 248\u001b[0m, in \u001b[0;36mevaluate_with_projection\u001b[0;34m(model, tokenizer, dataset, stop_at_layer, projection, max_samples)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Get next token\u001b[39;00m\n\u001b[1;32m    247\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 248\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([generated_ids, next_token], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id \u001b[38;5;129;01mor\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(next_token[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"projection_plots\", exist_ok=True)\n",
    "os.makedirs(\"projection_models\", exist_ok=True)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "    \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Get model config\n",
    "num_layers = model.config.num_hidden_layers\n",
    "hidden_size = model.config.hidden_size\n",
    "print(f\"Model has {num_layers} layers, hidden size {hidden_size}\")\n",
    "\n",
    "# Generation config\n",
    "GEN_CFG = GenerationConfig(max_new_tokens=3, do_sample=False, max_length=None)\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"Load the counting dataset\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} examples from dataset\")\n",
    "    return data\n",
    "\n",
    "def extract_int(text: str):\n",
    "    \"\"\"Return first integer, or -1 if none found.\"\"\"\n",
    "    INT_RE = re.compile(r\"\\d+\")\n",
    "    m = INT_RE.search(text)\n",
    "    return int(m.group()) if m else -1\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    \"\"\"Projects from early layer representations to final layer representations\"\"\"\n",
    "    def __init__(self, hidden_size, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        # Multi-layer projection with residual connection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "        )\n",
    "        \n",
    "        # Learnable residual weight\n",
    "        if use_residual:\n",
    "            self.residual_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        projected = self.projection(hidden_states)\n",
    "        if self.use_residual:\n",
    "            # Weighted sum of original and projected\n",
    "            return self.residual_weight * hidden_states + (1 - self.residual_weight) * projected\n",
    "        return projected\n",
    "\n",
    "def collect_hidden_states(model, tokenizer, dataset, source_layer, target_layer, num_samples=2000):\n",
    "    \"\"\"Collect hidden states from source and target layers\"\"\"\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    \n",
    "    source_hiddens = []\n",
    "    target_hiddens = []\n",
    "    position_indices = []  # Track which token positions we're collecting\n",
    "    \n",
    "    print(f\"Collecting hidden states from layers {source_layer} and {target_layer}...\")\n",
    "    \n",
    "    for i, example in enumerate(tqdm(dataset[:num_samples], desc=\"Collecting\")):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        prompt = example['prompt']\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            \n",
    "            # Get representations from source and target layers\n",
    "            source_hidden = outputs.hidden_states[source_layer]\n",
    "            target_hidden = outputs.hidden_states[target_layer]\n",
    "            \n",
    "            # Collect representations from multiple positions (not just the last token)\n",
    "            # This gives us more training data\n",
    "            seq_len = source_hidden.shape[1]\n",
    "            \n",
    "            # Use last 5 tokens (or all if sequence is shorter)\n",
    "            num_positions = min(5, seq_len)\n",
    "            for pos in range(seq_len - num_positions, seq_len):\n",
    "                source_hiddens.append(source_hidden[:, pos, :].cpu())\n",
    "                target_hiddens.append(target_hidden[:, pos, :].cpu())\n",
    "                position_indices.append(pos - seq_len)  # Negative indexing from end\n",
    "        \n",
    "        # Clear memory periodically\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    source_hiddens = torch.cat(source_hiddens, dim=0)\n",
    "    target_hiddens = torch.cat(target_hiddens, dim=0)\n",
    "    \n",
    "    print(f\"Collected {len(source_hiddens)} hidden state pairs\")\n",
    "    return source_hiddens, target_hiddens\n",
    "\n",
    "def train_projection(source_hiddens, target_hiddens, hidden_size, device, num_epochs=20, batch_size=64):\n",
    "    \"\"\"Train a projection from source to target representations\"\"\"\n",
    "    \n",
    "    # Ensure consistent dtype (use float32 for training stability)\n",
    "    source_hiddens = source_hiddens.float()\n",
    "    target_hiddens = target_hiddens.float()\n",
    "    \n",
    "    projection = ProjectionLayer(hidden_size, use_residual=True).to(device).float()\n",
    "    projection.train()\n",
    "    \n",
    "    # Move data to device\n",
    "    source_hiddens = source_hiddens.to(device)\n",
    "    target_hiddens = target_hiddens.to(device)\n",
    "    \n",
    "    # Create data loader\n",
    "    dataset = TensorDataset(source_hiddens, target_hiddens)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(projection.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    \n",
    "    print(f\"Training projection with {len(source_hiddens)} samples...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_source, batch_target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            projected = projection(batch_source)\n",
    "            loss = criterion(projected, batch_target)\n",
    "            \n",
    "            # Add cosine similarity loss to encourage directional alignment\n",
    "            cos_sim = nn.functional.cosine_similarity(projected, batch_target, dim=1).mean()\n",
    "            loss = loss - 0.1 * cos_sim  # Negative because we want to maximize similarity\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(projection.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    projection.eval()\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Projection Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('projection_plots/training_loss.png', dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    return projection\n",
    "\n",
    "def evaluate_with_projection(model, tokenizer, dataset, stop_at_layer, projection, max_samples=None):\n",
    "    \"\"\"Evaluate model with projection from early layer\"\"\"\n",
    "    \n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    projection = projection.to(device)\n",
    "    projection.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    \n",
    "    samples = dataset if max_samples is None else dataset[:max_samples]\n",
    "    \n",
    "    for example in tqdm(samples, desc=f\"Layer {stop_at_layer} + projection\"):\n",
    "        prompt = example['prompt']\n",
    "        true_answer = example['answer']\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = inputs['input_ids'].clone()\n",
    "            \n",
    "            for _ in range(GEN_CFG.max_new_tokens):\n",
    "                # Get hidden states at early layer\n",
    "                outputs = model(generated_ids, output_hidden_states=True, return_dict=True)\n",
    "                hidden_states = outputs.hidden_states[stop_at_layer]\n",
    "                \n",
    "                # Convert to float32 for projection\n",
    "                hidden_states = hidden_states.float()\n",
    "                \n",
    "                # Project to final layer space\n",
    "                hidden_states = projection(hidden_states)\n",
    "                \n",
    "                # Convert back to model's dtype (float16) and apply layer norm\n",
    "                hidden_states = hidden_states.to(model.dtype)\n",
    "                hidden_states = model.model.norm(hidden_states)\n",
    "                logits = model.lm_head(hidden_states)\n",
    "                \n",
    "                # Get next token\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == tokenizer.eos_token_id or tokenizer.decode(next_token[0]) == ')':\n",
    "                    break\n",
    "        \n",
    "        # Extract prediction\n",
    "        generation = tokenizer.decode(generated_ids[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "        predicted_answer = extract_int(generation)\n",
    "        \n",
    "        predictions.append({\n",
    "            'prompt': prompt,\n",
    "            'true_answer': true_answer,\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'generated_text': generation.strip(),\n",
    "            'correct': predicted_answer == true_answer\n",
    "        })\n",
    "        \n",
    "        if predicted_answer == true_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Debug first few\n",
    "        if total <= 5:\n",
    "            print(f\"\\nExample {total}:\")\n",
    "            print(f\"  Generated: '{generation.strip()}'\")\n",
    "            print(f\"  Predicted: {predicted_answer}, True: {true_answer}\")\n",
    "            print(f\"  Correct: {predicted_answer == true_answer}\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def run_projection_experiment(model, tokenizer, dataset, start_layer=10, end_layer=20, \n",
    "                            train_samples=2000, eval_samples=None):\n",
    "    \"\"\"Run the complete projection experiment\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Baseline accuracy (given)\n",
    "    BASELINE_ACCURACY = 0.73520\n",
    "    results['baseline'] = {\n",
    "        'layer': num_layers,\n",
    "        'accuracy': BASELINE_ACCURACY,\n",
    "        'predictions': []\n",
    "    }\n",
    "    \n",
    "    # Target layer is always the final layer\n",
    "    target_layer = num_layers\n",
    "    \n",
    "    # Collect hidden states for all layers we'll evaluate\n",
    "    print(\"\\nCollecting hidden states for projection training...\")\n",
    "    all_hidden_states = {}\n",
    "    \n",
    "    # Collect target layer hidden states once\n",
    "    _, target_hiddens = collect_hidden_states(\n",
    "        model, tokenizer, dataset, target_layer, target_layer, num_samples=train_samples\n",
    "    )\n",
    "    \n",
    "    # Train projections for each source layer\n",
    "    projections = {}\n",
    "    \n",
    "    for source_layer in range(start_layer, end_layer + 1):\n",
    "        print(f\"\\n=== Training projection for layer {source_layer} ===\")\n",
    "        \n",
    "        # Collect source layer hidden states\n",
    "        source_hiddens, _ = collect_hidden_states(\n",
    "            model, tokenizer, dataset, source_layer, target_layer, num_samples=train_samples\n",
    "        )\n",
    "        \n",
    "        # Train projection\n",
    "        device = model.model.embed_tokens.weight.device\n",
    "        projection = train_projection(\n",
    "            source_hiddens, target_hiddens, \n",
    "            hidden_size=model.config.hidden_size,\n",
    "            device=device,\n",
    "            num_epochs=20,\n",
    "            batch_size=64\n",
    "        )\n",
    "        \n",
    "        projections[source_layer] = projection\n",
    "        \n",
    "        # Save projection\n",
    "        torch.save(projection.state_dict(), f'projection_models/projection_layer_{source_layer}.pt')\n",
    "        \n",
    "        # Clear memory\n",
    "        del source_hiddens\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluate with projections\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for layer in range(start_layer, end_layer + 1):\n",
    "        print(f\"\\n=== Evaluating Layer {layer} with Projection ===\")\n",
    "        \n",
    "        accuracy, predictions = evaluate_with_projection(\n",
    "            model, tokenizer, dataset, \n",
    "            stop_at_layer=layer,\n",
    "            projection=projections[layer],\n",
    "            max_samples=eval_samples\n",
    "        )\n",
    "        \n",
    "        results[f'layer_{layer}'] = {\n",
    "            'layer': layer,\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy for Layer {layer} + Projection: {accuracy:.3%}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return results, projections\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"Create visualizations for the projection experiment\"\"\"\n",
    "    \n",
    "    # 1. Overall accuracy by layer\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    layers = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for key in sorted(results.keys()):\n",
    "        if key != 'baseline':\n",
    "            layer = results[key]['layer']\n",
    "            accuracy = results[key]['accuracy']\n",
    "            layers.append(layer)\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_acc = results['baseline']['accuracy']\n",
    "    \n",
    "    plt.plot(layers, accuracies, 'o-', linewidth=2, markersize=8, \n",
    "             label='Early Stop + Projection', color='blue')\n",
    "    plt.axhline(y=baseline_acc, color='red', linestyle='--', \n",
    "                label=f'Full Model ({num_layers} layers): {baseline_acc:.3%}')\n",
    "    \n",
    "    # Add 50% line for reference\n",
    "    plt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Random (50%)')\n",
    "    \n",
    "    plt.xlabel('Stop at Layer', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Counting Accuracy: Early Layers + Learned Projection', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Annotate best early layer\n",
    "    if accuracies:\n",
    "        max_acc = max(accuracies)\n",
    "        max_layer = layers[accuracies.index(max_acc)]\n",
    "        plt.annotate(f'Best: {max_acc:.3%} at layer {max_layer}',\n",
    "                    xy=(max_layer, max_acc),\n",
    "                    xytext=(max_layer + 1, max_acc - 0.1),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('projection_plots/accuracy_by_layer_with_projection.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Improvement over baseline early stopping\n",
    "    if os.path.exists('early_stop_results.json'):\n",
    "        # Load baseline early stop results if available\n",
    "        with open('early_stop_results.json', 'r') as f:\n",
    "            baseline_results = json.load(f)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        baseline_accs = []\n",
    "        projection_accs = []\n",
    "        comparison_layers = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            if f'layer_{layer}' in baseline_results.get('layer_results', {}):\n",
    "                baseline_acc = baseline_results['layer_results'][f'layer_{layer}']['accuracy']\n",
    "                projection_acc = results[f'layer_{layer}']['accuracy']\n",
    "                \n",
    "                baseline_accs.append(baseline_acc)\n",
    "                projection_accs.append(projection_acc)\n",
    "                comparison_layers.append(layer)\n",
    "        \n",
    "        if comparison_layers:\n",
    "            x = np.arange(len(comparison_layers))\n",
    "            width = 0.35\n",
    "            \n",
    "            plt.bar(x - width/2, baseline_accs, width, label='Direct Early Stop', color='orange', alpha=0.7)\n",
    "            plt.bar(x + width/2, projection_accs, width, label='With Projection', color='blue', alpha=0.7)\n",
    "            \n",
    "            plt.xlabel('Layer')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Comparison: Direct Early Stop vs. With Projection')\n",
    "            plt.xticks(x, comparison_layers)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('projection_plots/comparison_with_baseline.png', dpi=150)\n",
    "            plt.show()\n",
    "\n",
    "def save_results(results, output_file='projection_results.json'):\n",
    "    \"\"\"Save results to JSON\"\"\"\n",
    "    output = {\n",
    "        'summary': {\n",
    "            'baseline_accuracy': results['baseline']['accuracy'],\n",
    "            'best_projection_layer': None,\n",
    "            'best_projection_accuracy': 0\n",
    "        },\n",
    "        'layer_results': {}\n",
    "    }\n",
    "    \n",
    "    # Find best layer\n",
    "    for key, result in results.items():\n",
    "        if key != 'baseline':\n",
    "            if result['accuracy'] > output['summary']['best_projection_accuracy']:\n",
    "                output['summary']['best_projection_accuracy'] = result['accuracy']\n",
    "                output['summary']['best_projection_layer'] = result['layer']\n",
    "    \n",
    "    # Add layer results\n",
    "    for key, result in results.items():\n",
    "        output['layer_results'][key] = {\n",
    "            'layer': result['layer'],\n",
    "            'accuracy': result['accuracy'],\n",
    "            'num_correct': sum(1 for p in result['predictions'] if p['correct']),\n",
    "            'num_total': len(result['predictions'])\n",
    "        }\n",
    "    \n",
    "    # Save example predictions\n",
    "    output['example_predictions'] = {}\n",
    "    for key, result in results.items():\n",
    "        output['example_predictions'][key] = result['predictions'][:10]\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset_path = \"/net/scratch/slhleosun/counting-items-mechanisms/dataset.json\"\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    \n",
    "    # Run experiment\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROJECTION TRAINING EXPERIMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results, projections = run_projection_experiment(\n",
    "        model, tokenizer, dataset,\n",
    "        start_layer=10,\n",
    "        end_layer=20,\n",
    "        train_samples=2000,  # Number of examples for training projections\n",
    "        eval_samples=len(dataset)  # Evaluate on full dataset\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Baseline accuracy (full model): {results['baseline']['accuracy']:.3%}\")\n",
    "    \n",
    "    best_layer = None\n",
    "    best_accuracy = 0\n",
    "    for key, result in results.items():\n",
    "        if key != 'baseline' and result['accuracy'] > best_accuracy:\n",
    "            best_accuracy = result['accuracy']\n",
    "            best_layer = result['layer']\n",
    "    \n",
    "    if best_layer:\n",
    "        print(f\"Best projection layer: {best_layer} with accuracy {best_accuracy:.3%}\")\n",
    "        print(f\"Accuracy drop from baseline: {(results['baseline']['accuracy'] - best_accuracy):.3%}\")\n",
    "    \n",
    "    print(\"\\nAccuracy by layer:\")\n",
    "    for key in sorted(results.keys()):\n",
    "        if key != 'baseline':\n",
    "            layer = results[key]['layer']\n",
    "            acc = results[key]['accuracy']\n",
    "            print(f\"  Layer {layer}: {acc:.3%}\")\n",
    "    \n",
    "    print(\"\\nExperiment complete!\")\n",
    "    print(\"Outputs:\")\n",
    "    print(\"  - projection_plots/: Visualizations\")\n",
    "    print(\"  - projection_models/: Trained projections\") \n",
    "    print(\"  - projection_results.json: Detailed results\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fde5a9-4494-41a8-95c5-55d11e9fb170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cuda\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205b6f3ccab04f03a14d09dcd1c3fa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 32 layers, hidden size 4096, vocab size 128256\n",
      "Loaded 5000 examples from dataset\n",
      "\n",
      "Running early stop decode experiment...\n",
      "\n",
      "=== Evaluating Early Stop at Layer 10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 10:   0%|                                             | 1/5000 [00:17<23:51:32, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Example 1:\n",
      "  Prompt ending: ...ol, bear, lion, sheep, tangerine, peach]\n",
      "Answer: (\n",
      "  Generated: 'zielosenhower'\n",
      "  Extracted number: -1\n",
      "  True answer: 2\n",
      "  Correct: False\n",
      "\n",
      "Debug Example 2:\n",
      "  Prompt ending: ...[cello, plane, cherry, banana, bed, cow]\n",
      "Answer: (\n",
      "  Generated: 'ziel♠theless'\n",
      "  Extracted number: -1\n",
      "  True answer: 1\n",
      "  Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 10:   0%|                                              | 3/5000 [00:17<6:19:27,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Example 3:\n",
      "  Prompt ending: ...pet, horse, cabinet, table, desk, flute]\n",
      "Answer: (\n",
      "  Generated: 'zielosenhower'\n",
      "  Extracted number: -1\n",
      "  True answer: 3\n",
      "  Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 10: 100%|█████████████████████████████████████████████| 5000/5000 [07:05<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Early Stop at Layer 10: 0.000%\n",
      "\n",
      "=== Evaluating Early Stop at Layer 11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 11:   0%|                                              | 1/5000 [00:02<3:41:40,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Example 1:\n",
      "  Prompt ending: ...ol, bear, lion, sheep, tangerine, peach]\n",
      "Answer: (\n",
      "  Generated: '.nihkáchLBL'\n",
      "  Extracted number: -1\n",
      "  True answer: 2\n",
      "  Correct: False\n",
      "\n",
      "Debug Example 2:\n",
      "  Prompt ending: ...[cello, plane, cherry, banana, bed, cow]\n",
      "Answer: (\n",
      "  Generated: '.nihkáchLBL'\n",
      "  Extracted number: -1\n",
      "  True answer: 1\n",
      "  Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 11:   0%|                                              | 3/5000 [00:02<1:05:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Example 3:\n",
      "  Prompt ending: ...pet, horse, cabinet, table, desk, flute]\n",
      "Answer: (\n",
      "  Generated: '.nihkáchLBL'\n",
      "  Extracted number: -1\n",
      "  True answer: 3\n",
      "  Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 11:  54%|████████████████████████▎                    | 2701/5000 [03:44<03:10, 12.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 469\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExperiment complete! Results saved to early_stop_plots/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 469\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 424\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Run experiment\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning early stop decode experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 424\u001b[0m results \u001b[38;5;241m=\u001b[39m run_early_stop_experiment(\n\u001b[1;32m    425\u001b[0m     model, tokenizer, dataset, \n\u001b[1;32m    426\u001b[0m     start_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m    427\u001b[0m     end_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    428\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 214\u001b[0m, in \u001b[0;36mrun_early_stop_experiment\u001b[0;34m(model, tokenizer, dataset, start_layer, end_layer)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Evaluating Early Stop at Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m     accuracy, predictions \u001b[38;5;241m=\u001b[39m evaluate_early_stop(\n\u001b[1;32m    215\u001b[0m         model, tokenizer, dataset, layer, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m: layer,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions\n\u001b[1;32m    221\u001b[0m     }\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy for Early Stop at Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m, in \u001b[0;36mevaluate_early_stop\u001b[0;34m(base_model, tokenizer, dataset, stop_at_layer, max_samples)\u001b[0m\n\u001b[1;32m    133\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GEN_CFG\u001b[38;5;241m.\u001b[39mmax_new_tokens):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Get model outputs with hidden states\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m base_model(\n\u001b[1;32m    138\u001b[0m         generated_ids,\n\u001b[1;32m    139\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Get hidden states at the stop layer (layer outputs are 0-indexed, but we count from 1)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# hidden_states[0] is embeddings, hidden_states[1] is layer 0 output, etc.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[stop_at_layer]\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:831\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    832\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    833\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    834\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    835\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    836\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    837\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    838\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    839\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    840\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    841\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:589\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    578\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m         position_embeddings,\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    590\u001b[0m         hidden_states,\n\u001b[1;32m    591\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    592\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    593\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    594\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    595\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    596\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    597\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    601\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:347\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    346\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    348\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/slhleosun/miniconda3/envs/fixed_ai_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     70\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     71\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 72\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set device - but note that model might be distributed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"early_stop_plots\", exist_ok=True)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Get model config\n",
    "num_layers = model.config.num_hidden_layers\n",
    "hidden_size = model.config.hidden_size\n",
    "vocab_size = model.config.vocab_size\n",
    "print(f\"Model has {num_layers} layers, hidden size {hidden_size}, vocab size {vocab_size}\")\n",
    "\n",
    "# Generation config\n",
    "GEN_CFG = GenerationConfig(max_new_tokens=3, do_sample=False, max_length=None)\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"Load the counting dataset\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} examples from dataset\")\n",
    "    return data\n",
    "\n",
    "def extract_int(text: str):\n",
    "    \"\"\"Return first integer inside parentheses, or None.\"\"\"\n",
    "    INT_RE = re.compile(r\"\\d+\")\n",
    "    m = INT_RE.search(text)\n",
    "    return int(m.group()) if m else -1\n",
    "\n",
    "class EarlyStopLlamaModel(torch.nn.Module):\n",
    "    \"\"\"Wrapper to allow early stopping at specific layers\"\"\"\n",
    "    def __init__(self, base_model, stop_at_layer=None):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.stop_at_layer = stop_at_layer if stop_at_layer is not None else base_model.config.num_hidden_layers\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # Use the base model's forward method but intercept at the right layer\n",
    "        # We'll use a hook to stop execution at the desired layer\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            # Capture the output and prevent further processing\n",
    "            outputs.append(output[0])  # hidden states\n",
    "            return output\n",
    "        \n",
    "        # Register hook on the target layer\n",
    "        if self.stop_at_layer < self.base_model.config.num_hidden_layers:\n",
    "            hook = self.base_model.model.layers[self.stop_at_layer - 1].register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Run normal forward pass - it will process up to our hook\n",
    "            with torch.no_grad():\n",
    "                # Get model outputs\n",
    "                model_outputs = self.base_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Get hidden states at the stop layer\n",
    "                if outputs:\n",
    "                    # Use the captured hidden states from our hook\n",
    "                    hidden_states = outputs[0]\n",
    "                else:\n",
    "                    # If stop_at_layer == num_layers, use the final hidden states\n",
    "                    hidden_states = model_outputs.hidden_states[self.stop_at_layer]\n",
    "                \n",
    "                # Apply final layer norm\n",
    "                hidden_states = self.base_model.model.norm(hidden_states)\n",
    "                \n",
    "                # Get logits\n",
    "                logits = self.base_model.lm_head(hidden_states)\n",
    "                \n",
    "                return logits\n",
    "                \n",
    "        finally:\n",
    "            # Remove hook\n",
    "            if self.stop_at_layer < self.base_model.config.num_hidden_layers:\n",
    "                hook.remove()\n",
    "\n",
    "def evaluate_early_stop(base_model, tokenizer, dataset, stop_at_layer, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy when stopping at a specific layer\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    \n",
    "    samples = dataset if max_samples is None else dataset[:max_samples]\n",
    "    \n",
    "    for example in tqdm(samples, desc=f\"Layer {stop_at_layer}\"):\n",
    "        prompt = example['prompt']\n",
    "        true_answer = example['answer']\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        # Move to the same device as the model's embeddings\n",
    "        device = base_model.model.embed_tokens.weight.device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with early stop\n",
    "        with torch.no_grad():\n",
    "            # Simple greedy generation\n",
    "            generated_ids = inputs['input_ids'].clone()\n",
    "            \n",
    "            for _ in range(GEN_CFG.max_new_tokens):\n",
    "                # Get model outputs with hidden states\n",
    "                outputs = base_model(\n",
    "                    generated_ids,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Get hidden states at the stop layer (layer outputs are 0-indexed, but we count from 1)\n",
    "                # hidden_states[0] is embeddings, hidden_states[1] is layer 0 output, etc.\n",
    "                hidden_states = outputs.hidden_states[stop_at_layer]\n",
    "                \n",
    "                # Apply final layer norm\n",
    "                hidden_states = base_model.model.norm(hidden_states)\n",
    "                \n",
    "                # Get logits\n",
    "                logits = base_model.lm_head(hidden_states)\n",
    "                \n",
    "                # Get next token\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                # Ensure next_token is on the same device as generated_ids\n",
    "                next_token = next_token.to(generated_ids.device)\n",
    "                \n",
    "                # Append to sequence\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                # Check if we hit EOS or closing parenthesis\n",
    "                if next_token.item() == tokenizer.eos_token_id or tokenizer.decode(next_token[0]) == ')':\n",
    "                    break\n",
    "        \n",
    "        # Decode and extract prediction\n",
    "        generation = tokenizer.decode(generated_ids[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "        predicted_answer = extract_int(generation)\n",
    "        \n",
    "        predictions.append({\n",
    "            'prompt': prompt,\n",
    "            'true_answer': true_answer,\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'generated_text': generation.strip(),\n",
    "            'correct': predicted_answer == true_answer\n",
    "        })\n",
    "        \n",
    "        if predicted_answer == true_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Debug first few examples\n",
    "        if total <= 3:\n",
    "            print(f\"\\nDebug Example {total}:\")\n",
    "            print(f\"  Prompt ending: ...{prompt[-50:]}\")\n",
    "            print(f\"  Generated: '{generation.strip()}'\")\n",
    "            print(f\"  Extracted number: {predicted_answer}\")\n",
    "            print(f\"  True answer: {true_answer}\")\n",
    "            print(f\"  Correct: {predicted_answer == true_answer}\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, predictions\n",
    "\n",
    "\n",
    "\n",
    "def run_early_stop_experiment(model, tokenizer, dataset, start_layer=10, end_layer=20):\n",
    "    \"\"\"\n",
    "    Run early stop experiment across multiple layers\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Use provided baseline accuracy\n",
    "    BASELINE_ACCURACY = 0.73520\n",
    "    results['baseline'] = {\n",
    "        'layer': num_layers,\n",
    "        'accuracy': BASELINE_ACCURACY,\n",
    "        'predictions': []  # Empty since we're not running it\n",
    "    }\n",
    "    \n",
    "    # Run for specified layer range\n",
    "    for layer in range(start_layer, end_layer + 1):\n",
    "        print(f\"\\n=== Evaluating Early Stop at Layer {layer} ===\")\n",
    "        accuracy, predictions = evaluate_early_stop(\n",
    "            model, tokenizer, dataset, layer, max_samples=len(dataset)\n",
    "        )\n",
    "        results[f'layer_{layer}'] = {\n",
    "            'layer': layer,\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "        print(f\"Accuracy for Early Stop at Layer {layer}: {accuracy:.3%}\")\n",
    "        \n",
    "        # Clear memory periodically\n",
    "        if layer % 5 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results_by_category(results, dataset):\n",
    "    \"\"\"\n",
    "    Analyze results broken down by category and list length\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        layer = result['layer']\n",
    "        predictions = result['predictions']\n",
    "        \n",
    "        # Analyze by category\n",
    "        category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        length_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            if i < len(dataset):\n",
    "                category = dataset[i]['type']\n",
    "                list_length = dataset[i]['list_length']\n",
    "                \n",
    "                category_stats[category]['total'] += 1\n",
    "                length_stats[list_length]['total'] += 1\n",
    "                \n",
    "                if pred['correct']:\n",
    "                    category_stats[category]['correct'] += 1\n",
    "                    length_stats[list_length]['correct'] += 1\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        category_acc = {cat: stats['correct']/stats['total'] \n",
    "                       for cat, stats in category_stats.items() if stats['total'] > 0}\n",
    "        length_acc = {length: stats['correct']/stats['total'] \n",
    "                     for length, stats in length_stats.items() if stats['total'] > 0}\n",
    "        \n",
    "        analysis[key] = {\n",
    "            'layer': layer,\n",
    "            'overall_accuracy': result['accuracy'],\n",
    "            'category_accuracy': category_acc,\n",
    "            'length_accuracy': length_acc\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def visualize_results(results, analysis):\n",
    "    \"\"\"\n",
    "    Create visualizations for the early stop experiment\n",
    "    \"\"\"\n",
    "    # 1. Overall accuracy by layer\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    layers = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for key in sorted(results.keys()):\n",
    "        if key != 'baseline':\n",
    "            layer = results[key]['layer']\n",
    "            accuracy = results[key]['accuracy']\n",
    "            layers.append(layer)\n",
    "            accuracies.append(accuracy)\n",
    "    \n",
    "    # Add baseline\n",
    "    baseline_acc = results['baseline']['accuracy']\n",
    "    \n",
    "    plt.plot(layers, accuracies, 'o-', linewidth=2, markersize=8, label='Early Stop')\n",
    "    plt.axhline(y=baseline_acc, color='red', linestyle='--', \n",
    "                label=f'Full Model ({num_layers} layers): {baseline_acc:.3%}')\n",
    "    \n",
    "    plt.xlabel('Stop at Layer', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Counting Accuracy with Early Layer Decoding', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    if accuracies:\n",
    "        max_early_acc = max(accuracies)\n",
    "        max_early_layer = layers[accuracies.index(max_early_acc)]\n",
    "        plt.annotate(f'Best: {max_early_acc:.3f} at layer {max_early_layer}',\n",
    "                    xy=(max_early_layer, max_early_acc),\n",
    "                    xytext=(max_early_layer + 1, max_early_acc - 0.05),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('early_stop_plots/accuracy_by_layer.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Heatmap of accuracy by category and layer\n",
    "    categories = sorted(set(cat for a in analysis.values() \n",
    "                           for cat in a['category_accuracy'].keys()))\n",
    "    \n",
    "    if categories:\n",
    "        accuracy_matrix = []\n",
    "        layer_labels = []\n",
    "        \n",
    "        for key in sorted(results.keys()):\n",
    "            if key != 'baseline' and key in analysis:\n",
    "                layer = analysis[key]['layer']\n",
    "                layer_labels.append(f'Layer {layer}')\n",
    "                row = [analysis[key]['category_accuracy'].get(cat, 0) for cat in categories]\n",
    "                accuracy_matrix.append(row)\n",
    "        \n",
    "        if accuracy_matrix:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(np.array(accuracy_matrix).T, \n",
    "                       xticklabels=layer_labels,\n",
    "                       yticklabels=categories,\n",
    "                       annot=True, fmt='.3f',\n",
    "                       cmap='RdYlGn',\n",
    "                       vmin=0, vmax=1,\n",
    "                       cbar_kws={'label': 'Accuracy'})\n",
    "            plt.xlabel('Layer')\n",
    "            plt.ylabel('Category')\n",
    "            plt.title('Counting Accuracy by Category and Early Stop Layer')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('early_stop_plots/accuracy_by_category_heatmap.png', dpi=150)\n",
    "            plt.show()\n",
    "    \n",
    "    # 3. Error analysis - where do errors occur?\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (key, result) in enumerate(results.items()):\n",
    "        if key != 'baseline' and i < 5:  # Plot first 5 layers\n",
    "            layer = result['layer']\n",
    "            predictions = result['predictions']\n",
    "            \n",
    "            errors = []\n",
    "            for pred in predictions:\n",
    "                if not pred['correct'] and pred['predicted_answer'] != -1:\n",
    "                    error = pred['predicted_answer'] - pred['true_answer']\n",
    "                    errors.append(error)\n",
    "            \n",
    "            if errors:\n",
    "                plt.subplot(2, 3, i + 1)\n",
    "                plt.hist(errors, bins=20, alpha=0.7, edgecolor='black')\n",
    "                plt.xlabel('Prediction Error')\n",
    "                plt.ylabel('Count')\n",
    "                plt.title(f'Layer {layer}')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Distribution of Prediction Errors by Layer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('early_stop_plots/error_distribution.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def save_detailed_results(results, analysis, output_file='early_stop_results.json'):\n",
    "    \"\"\"\n",
    "    Save detailed results to JSON file\n",
    "    \"\"\"\n",
    "    # Convert to serializable format\n",
    "    output = {\n",
    "        'summary': {\n",
    "            'baseline_accuracy': results['baseline']['accuracy'],\n",
    "            'best_early_stop_layer': None,\n",
    "            'best_early_stop_accuracy': 0\n",
    "        },\n",
    "        'layer_results': {},\n",
    "        'analysis': analysis\n",
    "    }\n",
    "    \n",
    "    # Find best early stop layer\n",
    "    for key, result in results.items():\n",
    "        if key != 'baseline':\n",
    "            if result['accuracy'] > output['summary']['best_early_stop_accuracy']:\n",
    "                output['summary']['best_early_stop_accuracy'] = result['accuracy']\n",
    "                output['summary']['best_early_stop_layer'] = result['layer']\n",
    "    \n",
    "    # Add layer results (without full predictions to save space)\n",
    "    for key, result in results.items():\n",
    "        output['layer_results'][key] = {\n",
    "            'layer': result['layer'],\n",
    "            'accuracy': result['accuracy'],\n",
    "            'num_correct': sum(1 for p in result['predictions'] if p['correct']),\n",
    "            'num_total': len(result['predictions'])\n",
    "        }\n",
    "    \n",
    "    # Add some example predictions\n",
    "    output['example_predictions'] = {}\n",
    "    for key, result in results.items():\n",
    "        # Save first 5 predictions as examples\n",
    "        output['example_predictions'][key] = result['predictions'][:5]\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDetailed results saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset_path = \"/net/scratch/slhleosun/counting-items-mechanisms/dataset.json\"\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    \n",
    "    # Run experiment\n",
    "    print(\"\\nRunning early stop decode experiment...\")\n",
    "    results = run_early_stop_experiment(\n",
    "        model, tokenizer, dataset, \n",
    "        start_layer=10, \n",
    "        end_layer=20\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nAnalyzing results...\")\n",
    "    analysis = analyze_results_by_category(results, dataset)\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    visualize_results(results, analysis)\n",
    "    \n",
    "    # Save results\n",
    "    save_detailed_results(results, analysis)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Baseline accuracy (all layers): {results['baseline']['accuracy']:.3%}\")\n",
    "    \n",
    "    best_layer = None\n",
    "    best_accuracy = 0\n",
    "    for key, result in results.items():\n",
    "        if key != 'baseline' and result['accuracy'] > best_accuracy:\n",
    "            best_accuracy = result['accuracy']\n",
    "            best_layer = result['layer']\n",
    "    \n",
    "    if best_layer:\n",
    "        print(f\"Best early stop layer: {best_layer} with accuracy {best_accuracy:.3%}\")\n",
    "        print(f\"Accuracy drop from baseline: {(results['baseline']['accuracy'] - best_accuracy):.3%}\")\n",
    "    \n",
    "    # Print accuracy progression\n",
    "    print(\"\\nAccuracy by layer:\")\n",
    "    for key in sorted(results.keys()):\n",
    "        if key != 'baseline':\n",
    "            layer = results[key]['layer']\n",
    "            acc = results[key]['accuracy']\n",
    "            print(f\"  Layer {layer}: {acc:.3%}\")\n",
    "    a\n",
    "    print(\"\\nExperiment complete! Results saved to early_stop_plots/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7267308-8a43-4604-99a1-65661aa2f435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixed_ai_env",
   "language": "python",
   "name": "fixed_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
